{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Download model and dataset with Github"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!rm -rf /kaggle/working/easyocr-trainer\n",
    "!git clone https://github.com/cohui521/easyocr-trainer.git\n",
    "!cd /kaggle/working/easyocr-trainer\n",
    "!mv easyocr-trainer/* ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "Install requirements"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install lmdb pillow nltk natsort pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:34:40.324542Z",
     "start_time": "2024-12-19T16:34:38.304992Z"
    }
   },
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "from train import train\n",
    "from utils import AttrDict\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2024-12-19T16:34:41.979500Z",
     "start_time": "2024-12-19T16:34:41.975350Z"
    }
   },
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0
    ],
    "ExecuteTime": {
     "end_time": "2024-12-19T16:34:44.221273Z",
     "start_time": "2024-12-19T16:34:44.209792Z"
    }
   },
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T16:40:39.354859Z",
     "start_time": "2024-12-19T16:34:46.163209Z"
    }
   },
   "source": [
    "opt = get_config(\"config_files/god_bless_u.yaml\")\n",
    "train(opt, amp=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['all_data']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: all_data\n",
      "all_data/en_sample\n",
      "sub-directory:\t/en_sample\t num samples: 1\n",
      "num total samples of all_data: 1 x 1.0 (total_data_usage_ratio) = 1\n",
      "num samples of all_data per batch: 32 x 1.0 (batch_ratio) = 32\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 32 = 32\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data/en_sample\t dataset: /\n",
      "all_data/en_sample/\n",
      "sub-directory:\t/.\t num samples: 1\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n",
      "loading pretrained model from saved_models/english_g2.pth\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (FeatureExtraction): VGG_FeatureExtractor(\n",
      "      (ConvNet): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU(inplace=True)\n",
      "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "        (19): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n",
      "  )\n",
      ")\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 24832\n",
      "module.Prediction.bias 97\n",
      "Total Trainable Params: 3781345\n",
      "Trainable params num :  3781345\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 1.0\n",
      "    maximize: False\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "number: 0123456789\n",
      "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\n",
      "lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "experiment_name: en_filtered\n",
      "train_data: all_data\n",
      "valid_data: all_data/en_sample\n",
      "manualSeed: 1111\n",
      "workers: 6\n",
      "batch_size: 32\n",
      "num_iter: 300000\n",
      "valInterval: 20000\n",
      "saved_model: saved_models/english_g2.pth\n",
      "FT: False\n",
      "optim: False\n",
      "lr: 1.0\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['all_data']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 34\n",
      "imgH: 64\n",
      "imgW: 600\n",
      "rgb: False\n",
      "contrast_adjust: False\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "num_class: 97\n",
      "---------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\dataset.py:116\u001B[0m, in \u001B[0;36mBatch_Balanced_Dataset.get_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# image, text = data_loader_iter.next()\u001B[39;00m\n\u001B[1;32m--> 116\u001B[0m     image, text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_loader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m     balanced_batch_images\u001B[38;5;241m.\u001B[39mappend(image)\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1438\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown_workers()\n\u001B[1;32m-> 1438\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m \u001B[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m \n\u001B[0;32m   1442\u001B[0m \u001B[38;5;66;03m# Check if the next sample has already been generated\u001B[39;00m\n",
      "\u001B[1;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m opt \u001B[38;5;241m=\u001B[39m get_config(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig_files/en_fine_tunning_config.yaml\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mamp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\train.py:203\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(opt, show_number, amp)\u001B[0m\n\u001B[0;32m    201\u001B[0m     scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     image_tensors, labels \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    204\u001B[0m     image \u001B[38;5;241m=\u001B[39m image_tensors\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    205\u001B[0m     text, length \u001B[38;5;241m=\u001B[39m converter\u001B[38;5;241m.\u001B[39mencode(labels, batch_max_length\u001B[38;5;241m=\u001B[39mopt\u001B[38;5;241m.\u001B[39mbatch_max_length)\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\dataset.py:121\u001B[0m, in \u001B[0;36mBatch_Balanced_Dataset.get_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader_iter_list[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_loader_list[i])\n\u001B[1;32m--> 121\u001B[0m     image, text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataloader_iter_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m     balanced_batch_images\u001B[38;5;241m.\u001B[39mappend(image)\n\u001B[0;32m    123\u001B[0m     balanced_batch_texts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m text\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1448\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1449\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1451\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1400\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m   1401\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_thread\u001B[38;5;241m.\u001B[39mis_alive():\n\u001B[1;32m-> 1402\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1403\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1404\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mE:\\EasyOCR_Trains\\EasyOCR-Trainer\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1230\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m   1231\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m   1232\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1240\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m   1241\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m   1242\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1243\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m   1245\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1246\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m   1247\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m   1248\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32md:\\Python\\Python39\\lib\\queue.py:180\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    178\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m remaining \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[0;32m    179\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[1;32m--> 180\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_empty\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    181\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get()\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnot_full\u001B[38;5;241m.\u001B[39mnotify()\n",
      "File \u001B[1;32md:\\Python\\Python39\\lib\\threading.py:316\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 316\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    318\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
